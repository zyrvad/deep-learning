{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f5160a-9322-4fe8-aa9d-e8fe3eabc8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv(\"./data/movie.csv\")\n",
    "x = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "#tfidf: convert the words string inputs into numbers\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, norm=\"l1\")\n",
    "x_train_v = vectorizer.fit_transform(x_train)\n",
    "x_test_v = vectorizer.transform(x_test)\n",
    "\n",
    "#convert data to torch tensors\n",
    "x_train_tensor = torch.tensor(x_train_v.toarray(), dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_v.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "819d5807-b1ce-4db5-a59f-c680ac63fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e940155-f167-42dc-af13-2168db612d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Loss after mini-batch 1: 0.704226\n",
      "Loss after mini-batch 101: 0.694554\n",
      "Loss after mini-batch 201: 0.693947\n",
      "Loss after mini-batch 301: 0.693816\n",
      "Stopping epoch 1 early after 351 batches due to no improvement.\n",
      "Epoch 1 finished after 351 batches.\n",
      "Starting Epoch 2\n",
      "Loss after mini-batch 1: 0.693609\n",
      "Loss after mini-batch 101: 0.693556\n",
      "Loss after mini-batch 201: 0.693637\n",
      "Loss after mini-batch 301: 0.693718\n",
      "Stopping epoch 2 early after 359 batches due to no improvement.\n",
      "Epoch 2 finished after 359 batches.\n",
      "Starting Epoch 3\n",
      "Loss after mini-batch 1: 0.694988\n",
      "Loss after mini-batch 101: 0.694353\n",
      "Loss after mini-batch 201: 0.694090\n",
      "Stopping epoch 3 early after 204 batches due to no improvement.\n",
      "Epoch 3 finished after 204 batches.\n",
      "Starting Epoch 4\n",
      "Loss after mini-batch 1: 0.694248\n",
      "Loss after mini-batch 101: 0.693259\n",
      "Stopping epoch 4 early after 169 batches due to no improvement.\n",
      "Epoch 4 finished after 169 batches.\n",
      "Starting Epoch 5\n",
      "Loss after mini-batch 1: 0.691648\n",
      "Loss after mini-batch 101: 0.693004\n",
      "Loss after mini-batch 201: 0.693104\n",
      "Stopping epoch 5 early after 217 batches due to no improvement.\n",
      "Epoch 5 finished after 217 batches.\n",
      "Starting Epoch 6\n",
      "Loss after mini-batch 1: 0.693902\n",
      "Loss after mini-batch 101: 0.693355\n",
      "Loss after mini-batch 201: 0.693194\n",
      "Loss after mini-batch 301: 0.693141\n",
      "Stopping epoch 6 early after 345 batches due to no improvement.\n",
      "Epoch 6 finished after 345 batches.\n",
      "Starting Epoch 7\n",
      "Loss after mini-batch 1: 0.692441\n",
      "Loss after mini-batch 101: 0.692797\n",
      "Loss after mini-batch 201: 0.693087\n",
      "Stopping epoch 7 early after 207 batches due to no improvement.\n",
      "Epoch 7 finished after 207 batches.\n",
      "Starting Epoch 8\n",
      "Loss after mini-batch 1: 0.697286\n",
      "Loss after mini-batch 101: 0.693137\n",
      "Stopping epoch 8 early after 186 batches due to no improvement.\n",
      "Epoch 8 finished after 186 batches.\n",
      "Starting Epoch 9\n",
      "Loss after mini-batch 1: 0.694215\n",
      "Loss after mini-batch 101: 0.693053\n",
      "Stopping epoch 9 early after 157 batches due to no improvement.\n",
      "Epoch 9 finished after 157 batches.\n",
      "Starting Epoch 10\n",
      "Loss after mini-batch 1: 0.693717\n",
      "Loss after mini-batch 101: 0.693255\n",
      "Stopping epoch 10 early after 152 batches due to no improvement.\n",
      "Epoch 10 finished after 152 batches.\n",
      "Starting Epoch 11\n",
      "Loss after mini-batch 1: 0.690991\n",
      "Loss after mini-batch 101: 0.693069\n",
      "Loss after mini-batch 201: 0.693144\n",
      "Stopping epoch 11 early after 214 batches due to no improvement.\n",
      "Epoch 11 finished after 214 batches.\n",
      "Starting Epoch 12\n",
      "Loss after mini-batch 1: 0.691449\n",
      "Loss after mini-batch 101: 0.693178\n",
      "Loss after mini-batch 201: 0.693104\n",
      "Stopping epoch 12 early after 207 batches due to no improvement.\n",
      "Epoch 12 finished after 207 batches.\n",
      "Starting Epoch 13\n",
      "Loss after mini-batch 1: 0.693194\n",
      "Loss after mini-batch 101: 0.692979\n",
      "Loss after mini-batch 201: 0.693158\n",
      "Stopping epoch 13 early after 217 batches due to no improvement.\n",
      "Epoch 13 finished after 217 batches.\n",
      "Starting Epoch 14\n",
      "Loss after mini-batch 1: 0.693926\n",
      "Loss after mini-batch 101: 0.693522\n",
      "Stopping epoch 14 early after 188 batches due to no improvement.\n",
      "Epoch 14 finished after 188 batches.\n",
      "Starting Epoch 15\n",
      "Loss after mini-batch 1: 0.693118\n",
      "Loss after mini-batch 101: 0.693179\n",
      "Stopping epoch 15 early after 126 batches due to no improvement.\n",
      "Epoch 15 finished after 126 batches.\n",
      "Starting Epoch 16\n",
      "Loss after mini-batch 1: 0.696587\n",
      "Loss after mini-batch 101: 0.693205\n",
      "Stopping epoch 16 early after 193 batches due to no improvement.\n",
      "Epoch 16 finished after 193 batches.\n",
      "Starting Epoch 17\n",
      "Loss after mini-batch 1: 0.693315\n",
      "Loss after mini-batch 101: 0.693143\n",
      "Loss after mini-batch 201: 0.693219\n",
      "Stopping epoch 17 early after 238 batches due to no improvement.\n",
      "Epoch 17 finished after 238 batches.\n",
      "Starting Epoch 18\n",
      "Loss after mini-batch 1: 0.692966\n",
      "Loss after mini-batch 101: 0.693106\n",
      "Stopping epoch 18 early after 178 batches due to no improvement.\n",
      "Epoch 18 finished after 178 batches.\n",
      "Starting Epoch 19\n",
      "Loss after mini-batch 1: 0.691967\n",
      "Loss after mini-batch 101: 0.693292\n",
      "Stopping epoch 19 early after 166 batches due to no improvement.\n",
      "Epoch 19 finished after 166 batches.\n",
      "Starting Epoch 20\n",
      "Loss after mini-batch 1: 0.694672\n",
      "Loss after mini-batch 101: 0.693262\n",
      "Stopping epoch 20 early after 167 batches due to no improvement.\n",
      "Epoch 20 finished after 167 batches.\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "#defining mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "input_size = x_train_tensor.shape[1]\n",
    "mlp = MLP(input_size, 2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "epsilon = 1e-5\n",
    "prev_loss = float('inf')\n",
    "patience_batches = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    current_loss = 0.0\n",
    "    batch_no_improve = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(trainloader, 0):\n",
    "        inputs, targets = inputs.float(), targets.long()  # Ensure correct types\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        avg_loss = current_loss / (i + 1)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Loss after mini-batch {i+1}: {avg_loss:.6f}')\n",
    "\n",
    "        if abs(prev_loss - avg_loss) < epsilon:\n",
    "            batch_no_improve += 1\n",
    "            if batch_no_improve >= patience_batches:\n",
    "                print(f\"Stopping epoch {epoch+1} early after {i+1} batches due to no improvement.\")\n",
    "                break\n",
    "        else:\n",
    "            batch_no_improve = 0\n",
    "\n",
    "        prev_loss = avg_loss\n",
    "\n",
    "    scheduler.step()  # Reduce learning rate after step_size epochs\n",
    "    print(f\"Epoch {epoch+1} finished after {i+1} batches.\")\n",
    "\n",
    "print(\"Training has completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ab86e-844b-4e9f-b1ba-81996447acbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
